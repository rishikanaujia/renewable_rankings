# ğŸ¤– Phase 2: Parameter Agents - Getting Started

## âœ… What Was Just Built

You now have a **complete, working parameter agent system**!

### Files Created

```
src/agents/
â”œâ”€â”€ base_agent.py                    # âœ… Abstract base class
â”œâ”€â”€ parameter_agents/
â”‚   â”œâ”€â”€ __init__.py                 # âœ… Agent registry
â”‚   â””â”€â”€ ambition_agent.py           # âœ… First complete agent
â””â”€â”€ agent_service.py                 # âœ… Service layer

scripts/
â””â”€â”€ demo_ambition_agent.py           # âœ… Interactive demo

docs/
â””â”€â”€ AGENT_SYSTEM_GUIDE.md           # âœ… Complete documentation
```

---

## ğŸš€ Quick Start (5 Minutes)

### 1. Run the Demo

```bash
# From project root
python scripts/demo_ambition_agent.py
```

**You'll see:**
- âœ… Direct agent usage examples
- âœ… Convenience function usage
- âœ… Service layer integration
- âœ… Scoring rubric visualization
- âœ… All mock countries comparison

### 2. Test in Python REPL

```python
from src.agents.parameter_agents import AmbitionAgent

# Create agent
agent = AmbitionAgent()

# Analyze a country
result = agent.analyze("Brazil", "Q3 2024")

# View results
print(f"Score: {result.score}/10")
print(f"Justification: {result.justification}")
print(f"Confidence: {result.confidence}")
```

### 3. Test via Service Layer

```python
from src.agents.agent_service import agent_service

# Analyze single parameter
result = agent_service.analyze_parameter("ambition", "Germany")

# Analyze subcategory (currently only has ambition)
subcat = agent_service.analyze_subcategory("regulation", "USA")

# Full country analysis (when more agents are implemented)
# ranking = agent_service.analyze_country("Brazil")
```

---

## ğŸ“Š What the Ambition Agent Does

### Purpose
Analyzes government renewable energy ambition based on targeted installed capacity (solar PV + onshore wind + offshore wind) by 2030.

### Input
```python
country = "Brazil"
period = "Q3 2024"
```

### Output
```python
ParameterScore(
    parameter_name="Ambition",
    score=7.0,  # 1-10 scale
    justification="26.8 GW of renewable capacity targeted by 2030 "
                  "(solar PV: 15.0 GW, onshore wind: 10.8 GW, "
                  "offshore wind: 1.0 GW). High targets.",
    data_sources=["Brazil NDC 2024", "IRENA Statistics", ...],
    confidence=0.8  # 0-1 scale
)
```

### Scoring Rubric

| Score | GW Range | Example Countries |
|-------|----------|-------------------|
| 1-3 | < 10 GW | Small nations |
| 4-6 | 10-25 GW | Medium economies |
| 7-8 | 25-35 GW | Brazil, Vietnam |
| 9-10 | > 35 GW | USA, China, Germany |

---

## ğŸ—ï¸ Architecture Deep Dive

### Class Hierarchy

```
BaseParameterAgent (Abstract)
    â†“
AmbitionAgent (Concrete)
    â”œâ”€ SCORING_RUBRIC (10 levels)
    â”œâ”€ MOCK_DATA (10 countries)
    â”œâ”€ analyze() - Main entry point
    â”œâ”€ _fetch_data() - Data collection
    â”œâ”€ _calculate_score() - Apply rubric
    â””â”€ _generate_justification() - Create explanation
```

### Execution Flow

```
User: agent.analyze("Brazil", "Q3 2024")
    â†“
1. _fetch_data()
    â†’ Returns: {"total_gw": 26.8, "solar": 15.0, ...}
    â†“
2. _calculate_score()
    â†’ Applies rubric: 26.8 GW â†’ Score 7
    â†“
3. _validate_score()
    â†’ Ensures 1-10 range
    â†“
4. _generate_justification()
    â†’ Creates: "26.8 GW of renewable capacity..."
    â†“
5. _estimate_confidence()
    â†’ Calculates: 0.8 (80% confident)
    â†“
6. Return ParameterScore
```

### Agent Modes

**Currently Implemented:**
```python
AgentMode.MOCK  # Uses MOCK_DATA dictionary
```

**Coming in Phase 2:**
```python
AgentMode.RULE_BASED   # Queries PostgreSQL database
AgentMode.AI_POWERED   # Uses LLM for extraction
```

---

## ğŸ¯ Your Next Tasks

### Task 1: Understand the Code (30 minutes)

**Read these files in order:**
1. `src/agents/base_agent.py` - Base class (15 min)
2. `src/agents/parameter_agents/ambition_agent.py` - Implementation (10 min)
3. `src/agents/agent_service.py` - Integration (5 min)

**Key concepts to understand:**
- Abstract methods (`@abstractmethod`)
- Inheritance (`super().__init__()`)
- Type hints (`Dict[str, Any]`)
- Pydantic models (`ParameterScore`)
- Logging (`logger.info()`)

### Task 2: Modify Mock Data (15 minutes)

**Exercise:**
1. Open `src/agents/parameter_agents/ambition_agent.py`
2. Find the `MOCK_DATA` dictionary
3. Add a new country:

```python
MOCK_DATA = {
    # ... existing countries ...
    "France": {
        "total_gw": 73.0,
        "solar": 35.0,
        "onshore_wind": 33.0,
        "offshore_wind": 5.0
    }
}
```

4. Test it:
```python
from src.agents.parameter_agents import analyze_ambition

result = analyze_ambition("France")
print(f"France: {result.score}/10")  # Should be 8/10
```

### Task 3: Create Your Second Agent (2-3 hours)

**Choose one:**
- **Support Scheme** (Medium difficulty)
- **Track Record** (Medium difficulty)
- **Country Stability** (Easy - just uses ECR rating)

**Steps:**
1. Copy `ambition_agent.py` â†’ `your_agent.py`
2. Update class name
3. Define scoring rubric
4. Add mock data
5. Implement `_calculate_score()`
6. Implement `_generate_justification()`
7. Register in `__init__.py`
8. Test!

**See** `docs/AGENT_SYSTEM_GUIDE.md` for detailed instructions.

---

## ğŸ§ª Testing Your Agents

### Manual Testing

```python
# Test directly
from src.agents.parameter_agents import AmbitionAgent

agent = AmbitionAgent()

# Test scoring logic
data = {"total_gw": 27.0, "solar": 15, "onshore_wind": 11, "offshore_wind": 1}
score = agent._calculate_score(data, "Test Country", "Q3 2024")
assert score == 7.0

# Test justification
justification = agent._generate_justification(data, score, "Test", "Q3 2024")
assert "27.0 GW" in justification
```

### Unit Tests (Coming)

```bash
# Create tests/test_agents/test_ambition_agent.py
pytest tests/test_agents/
```

---

## ğŸ”— Integration with UI

### Current State

**UI â†’ mock_service â†’ Hardcoded Mock Data**

### Phase 2 Target

**UI â†’ ranking_service â†’ agent_service â†’ Agents**

### How to Enable Agents

**Option 1: Direct Replacement**

Edit `src/services/mock_service.py`:

```python
from ..agents.agent_service import agent_service

class MockRankingService:
    def get_country_ranking(self, country, period):
        # OLD: Return hardcoded mock data
        # NEW: Use agents!
        return agent_service.analyze_country(country, period)
```

**Option 2: Configuration Toggle**

Edit `config/app_config.yaml`:

```yaml
system:
  mock_mode: false  # Enable agents
  agent_mode: "mock"
```

Then in service:
```python
if config['system']['mock_mode']:
    return mock_data
else:
    return agent_service.analyze_country(country)
```

---

## ğŸ“ˆ Roadmap: All 21 Agents

### Level I: Critical (11 agents)

**Regulation (5 agents):**
- âœ… Ambition (DONE)
- ğŸ”„ Support Scheme
- ğŸ”„ Track Record
- ğŸ”„ Contract Terms
- ğŸ”„ Country Stability

**Profitability (4 agents):**
- ğŸ”„ Revenue Stream Stability
- ğŸ”„ Offtaker Status
- ğŸ”„ Expected Return
- ğŸ”„ Long-Term Interest Rates

**Accommodation (2 agents):**
- ğŸ”„ Status of Grid
- ğŸ”„ Ownership Hurdles

### Level II: Important (6 agents)

**Market Size & Fundamentals (4 agents):**
- ğŸ”„ Power Market Size
- ğŸ”„ Resource Availability
- ğŸ”„ Energy Dependence
- ğŸ”„ Renewables Penetration

**Competition & Ease (2 agents):**
- ğŸ”„ Ownership Consolidation
- ğŸ”„ Competitive Landscape

### Level III: Modifiers (1 agents)

**System/External (1 agents, could be combined):**
- ğŸ”„ Cannibalization
- ğŸ”„ Curtailment
- ğŸ”„ Queue Dynamics
- ğŸ”„ Supply Chain

**Estimated Effort:**
- Simple agents (e.g., Country Stability): 1-2 hours
- Medium agents (e.g., Support Scheme): 2-4 hours
- Complex agents (e.g., Track Record): 4-6 hours

**Total:** ~60-80 hours for all 21 agents

---

## ğŸ’¡ Tips & Best Practices

### 1. Start Simple
Build the simplest agents first (Country Stability, Power Market Size) to get comfortable with the pattern.

### 2. One Agent at a Time
Don't try to build multiple agents simultaneously. Complete one, test it, then move to the next.

### 3. Copy-Paste is OK
The agents are intentionally similar. Copy `ambition_agent.py` as your template.

### 4. Focus on Rubric
The hardest part is defining the scoring rubric. Get this right first.

### 5. Mock Data is Temporary
Don't spend too much time perfecting mock data. It's just for testing.

### 6. Test Incrementally
Test each method (`_fetch_data`, `_calculate_score`, etc.) individually.

### 7. Use Logging
Add `logger.debug()` statements to understand what's happening.

### 8. Read the Guide
`docs/AGENT_SYSTEM_GUIDE.md` has complete step-by-step instructions.

---

## ğŸ“ Learning Resources

### Key Concepts

**1. Abstract Base Classes**
```python
from abc import ABC, abstractmethod

class BaseAgent(ABC):
    @abstractmethod
    def analyze(self):
        pass  # Must be implemented by subclasses
```

**2. Type Hints**
```python
def analyze(self, country: str) -> ParameterScore:
    # Returns ParameterScore object
    pass
```

**3. Pydantic Models**
```python
from pydantic import BaseModel

class ParameterScore(BaseModel):
    score: float  # Auto-validated
    justification: str
```

**4. Logging**
```python
from core.logger import get_logger

logger = get_logger(__name__)
logger.info("Analysis starting...")
```

### External Resources

- **Python Abstract Classes:** https://docs.python.org/3/library/abc.html
- **Type Hints:** https://docs.python.org/3/library/typing.html
- **Pydantic:** https://docs.pydantic.dev/
- **Loguru:** https://github.com/Delgan/loguru

---

## â“ FAQ

**Q: Do I need LangChain for Phase 2?**  
A: No! Agents currently use simple Python logic. LangChain comes in Phase 3 for AI_POWERED mode.

**Q: Can I test agents without the UI?**  
A: Yes! Use the demo script or Python REPL.

**Q: What if I want to change the scoring rubric?**  
A: Just edit the `SCORING_RUBRIC` constant in the agent class.

**Q: How do I add more countries to mock data?**  
A: Add entries to the `MOCK_DATA` dictionary.

**Q: When should I switch from MOCK to RULE_BASED mode?**  
A: After implementing the database and data pipeline (Phase 2.2).

**Q: Can agents call other agents?**  
A: Yes, but avoid it. Keep agents independent. Use the service layer for orchestration.

**Q: How do I handle missing data?**  
A: Return default/conservative values and reduce confidence score.

---

## âœ… Success Checklist

After completing this phase, you should be able to:

- [ ] Run `python scripts/demo_ambition_agent.py` successfully
- [ ] Create a new parameter agent from scratch
- [ ] Explain how the BaseParameterAgent class works
- [ ] Modify scoring rubrics
- [ ] Add/modify mock data
- [ ] Test agents via Python REPL
- [ ] Understand the service layer integration
- [ ] Know the difference between agent modes

---

## ğŸš€ Ready to Build!

**You have everything you need:**
- âœ… Working example (AmbitionAgent)
- âœ… Base class to inherit from
- âœ… Service layer for integration
- âœ… Demo script to learn from
- âœ… Complete documentation

**Next steps:**
1. Run the demo script
2. Read the agent code
3. Create your second agent
4. Repeat 20 more times! ğŸ˜„

**Questions?** Check `docs/AGENT_SYSTEM_GUIDE.md`

**Happy coding! ğŸ¯**
